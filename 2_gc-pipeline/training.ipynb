{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea876282-7e0d-4a5e-be56-0b31f49f3992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install --quiet --upgrade pip\n",
    "\n",
    "# Install the dependencies.\n",
    "!pip install --quiet -r requirements_tf.txt\n",
    "\n",
    "# Restart the runtime by ending the process.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d9bc1e-c419-4350-a556-1a76cb21ab39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 00:34:18.814243: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-23 00:34:18.864143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "from importlib import reload\n",
    "from serving.constants import  BUCKET, IMG_SOURCE_PREFIX, HIST_DEST_PREFIX, NUM_BANDS, HIST_BINS_LIST, SCALE, CROP, MONTHS, IMAGE_BATCH, NUM_BINS\n",
    "from serving.common import list_blobs_with_prefix\n",
    "from serving.hist_training import create_hist_dataset, train_and_evaluate, create_data_sample, get_labels #, LstmModel\n",
    "from serving.data import check_blob_prefix_exists, batch_check_blobs, get_varied_labels\n",
    "import logging\n",
    "import io\n",
    "import itertools\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d0c01e-1906-4f98-9840-bd8e7ac5a307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(features, label):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'feature': _bytes_feature(tf.io.serialize_tensor(features)),\n",
    "        'label': _float_feature(label),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def save_dataset_to_gcp(dataset, bucket_name='vgnn', file_name='hist_dataset_medium.tfrecords'):\n",
    "    # Initialize GCP client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Create a local temporary file\n",
    "    local_file_name = 'temp_' + file_name\n",
    "    \n",
    "    # Write the dataset to the local file\n",
    "    with tf.io.TFRecordWriter(local_file_name) as writer:\n",
    "        for features, label in dataset:\n",
    "            example = serialize_example(features, label)\n",
    "            writer.write(example)\n",
    "    \n",
    "    # Upload the local file to GCS\n",
    "    blob = bucket.blob(f\"dataset/{file_name}\")\n",
    "    blob.upload_from_filename(local_file_name)\n",
    "\n",
    "    # Remove the local temporary file\n",
    "    os.remove(local_file_name)\n",
    "\n",
    "    print(f\"Dataset saved to gs://{bucket_name}/dataset/{file_name}\")\n",
    "    \n",
    "def parse_tfrecord_fn(example_proto):\n",
    "    # Define the features dictionary that matches the structure used when saving\n",
    "    feature_description = {\n",
    "        'feature': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([1],tf.float32)\n",
    "    }\n",
    "    \n",
    "    # Parse the input tf.Example proto using the feature description\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Decode the feature from the parsed example\n",
    "    feature = tf.io.parse_tensor(parsed_features['feature'], out_type=tf.float32)\n",
    "    label = parsed_features['label']\n",
    "    \n",
    "    return feature, label\n",
    "\n",
    "def load_dataset_from_gcp(bucket_name='vgnn', file_name='hist_dataset_medium.tfrecords'):\n",
    "    # Construct the full GCS path\n",
    "    gcs_path = f\"gs://{bucket_name}/dataset/{file_name}\"\n",
    "    \n",
    "    # Create a TFRecordDataset\n",
    "    dataset = tf.data.TFRecordDataset(gcs_path)\n",
    "    \n",
    "    # Parse the TFRecords\n",
    "    parsed_dataset = dataset.map(parse_tfrecord_fn)     \n",
    "    \n",
    "    return parsed_dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c83aac7-7acf-435f-8cc9-e24d51bf3a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "CREATE_DATASET = False\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "credentials, _ = google.auth.default()\n",
    "bucket_name = BUCKET\n",
    "directory_prefix = IMG_SOURCE_PREFIX\n",
    "output_prefix = HIST_DEST_PREFIX\n",
    "labels_data = \"labels_combined.npy\"\n",
    "labels_header = \"labels_header.npy\"\n",
    "batch_size = 64\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfcac68-58ca-4b85-9ac5-5fd97d9f5476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 00:34:24.012784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "if CREATE_DATASET:\n",
    "    # Generate all prefixes\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    labels_df = get_varied_labels(0,10)\n",
    "    labels = list(zip(labels_df[\"county_name\"],\n",
    "                      labels_df[\"county_ansi\"],\n",
    "                 labels_df[\"state_ansi\"],\n",
    "                 labels_df[\"year\"]))\n",
    "\n",
    "\n",
    "    prefixes_hist = [f'histograms/{NUM_BINS}_bcukets/{SCALE}/{county.capitalize()}_{fips}/{year}' \n",
    "                for county,_, fips, year in labels]\n",
    "\n",
    "    # Batch check all prefixes\n",
    "    results_hist = batch_check_blobs(bucket_name, prefixes_hist)\n",
    "\n",
    "    # Generate get_input_img_params based on results\n",
    "    get_input_img_params = [(county.capitalize(), fips, year)\n",
    "                for county, county_fips, fips, year in labels\n",
    "                if results_hist[f'histograms/{NUM_BINS}_bcukets/{SCALE}/{county.capitalize()}_{fips}/{year}']\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of items to process: {len(get_input_img_params)}\")\n",
    "    \n",
    "    end_time_dataset_info = time.perf_counter()\n",
    "    print(f\"Collecting dataset information: {end_time_dataset_info-start_time}\")\n",
    "    \n",
    "    dataset, input_shape = create_hist_dataset(get_input_img_params, \"labels_combined.npy\", \"labels_header.npy\")\n",
    "    end_time_dataset = time.perf_counter()\n",
    "    \n",
    "    print(f\"Building dataset: {(end_time_dataset - end_time_dataset_info)/60:.02} minutes\")\n",
    "    \n",
    "    # Save dataset\n",
    "    save_dataset_to_gcp(dataset, file_name = f\"{NUM_BINS}_buckets_training_test\")\n",
    "    print(\"Dataset created\")\n",
    "else:\n",
    "    dataset = load_dataset_from_gcp(file_name=f\"{NUM_BINS}_buckets_training_test\")\n",
    "    for features, label in dataset.take(1):\n",
    "        input_shape = tuple(features.shape)\n",
    "        label_shape = label.shape\n",
    "    print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5819d1-9cc4-4695-8524-7a129d184249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape :(3, 1664)\n",
      "dataset: <class 'tensorflow.python.data.ops.map_op._MapDataset'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 00:34:24.492292: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "for features, label in dataset.take(1):\n",
    "    input_shape = tuple(features.shape)\n",
    "\n",
    "print(f\"\"\"input_shape :{input_shape}\n",
    "dataset: {type(dataset)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a26a19-410d-4e78-8e7c-ead300283a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commodity_desc</th>\n",
       "      <th>reference_period_desc</th>\n",
       "      <th>year</th>\n",
       "      <th>state_ansi</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_ansi</th>\n",
       "      <th>county_name</th>\n",
       "      <th>target</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>CORN</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>2016</td>\n",
       "      <td>06</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>101</td>\n",
       "      <td>SUTTER</td>\n",
       "      <td>264.1</td>\n",
       "      <td>USDA_Corn_County_2016.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>CORN</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>2018</td>\n",
       "      <td>06</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>101</td>\n",
       "      <td>SUTTER</td>\n",
       "      <td>210.0</td>\n",
       "      <td>USDA_Corn_County_2018.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>CORN</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>2019</td>\n",
       "      <td>06</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>101</td>\n",
       "      <td>SUTTER</td>\n",
       "      <td>147.1</td>\n",
       "      <td>USDA_Corn_County_2019.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>CORN</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>2021</td>\n",
       "      <td>06</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>101</td>\n",
       "      <td>SUTTER</td>\n",
       "      <td>169.9</td>\n",
       "      <td>USDA_Corn_County_2021.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8791</th>\n",
       "      <td>CORN</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>2022</td>\n",
       "      <td>06</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>101</td>\n",
       "      <td>SUTTER</td>\n",
       "      <td>133.8</td>\n",
       "      <td>USDA_Corn_County_2022.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     commodity_desc reference_period_desc  year state_ansi  state_name  \\\n",
       "66             CORN                  YEAR  2016         06  CALIFORNIA   \n",
       "3045           CORN                  YEAR  2018         06  CALIFORNIA   \n",
       "4389           CORN                  YEAR  2019         06  CALIFORNIA   \n",
       "7314           CORN                  YEAR  2021         06  CALIFORNIA   \n",
       "8791           CORN                  YEAR  2022         06  CALIFORNIA   \n",
       "\n",
       "     county_ansi county_name target                source_file  \n",
       "66           101      SUTTER  264.1  USDA_Corn_County_2016.csv  \n",
       "3045         101      SUTTER  210.0  USDA_Corn_County_2018.csv  \n",
       "4389         101      SUTTER  147.1  USDA_Corn_County_2019.csv  \n",
       "7314         101      SUTTER  169.9  USDA_Corn_County_2021.csv  \n",
       "8791         101      SUTTER  133.8  USDA_Corn_County_2022.csv  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data = np.load(labels_data, allow_pickle=True)\n",
    "label_header = np.load(labels_header, allow_pickle=True)\n",
    "label_df = pd.DataFrame(label_data, columns=label_header)\n",
    "label_df[label_df[\"county_name\"]=='Sutter'.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0f55cb-f178-4cfc-a583-bacb35a5a5db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "  Features shape: (3, 1664)\n",
      "  Label shape: (1,)\n",
      "Sample 2:\n",
      "  Features shape: (3, 1664)\n",
      "  Label shape: (1,)\n",
      "Sample 3:\n",
      "  Features shape: (3, 1664)\n",
      "  Label shape: (1,)\n",
      "Sample 4:\n",
      "  Features shape: (3, 1664)\n",
      "  Label shape: (1,)\n",
      "Sample 5:\n",
      "  Features shape: (3, 1664)\n",
      "  Label shape: (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 00:34:30.831107: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# Function to check shapes\n",
    "def check_dataset_shapes(dataset, num_samples_to_check=5):\n",
    "    for i, (features, label) in enumerate(dataset.take(num_samples_to_check)):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Features shape: {features.shape}\")\n",
    "        print(f\"  Label shape: {label.shape}\")\n",
    "        if i == 0:\n",
    "            first_shape = features.shape\n",
    "        else:\n",
    "            if features.shape != first_shape:\n",
    "                print(\"Warning: Inconsistent feature shape detected!\")\n",
    "                break\n",
    "\n",
    "# Check the shapes of the first few samples\n",
    "check_dataset_shapes(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbcd2b7-f7fe-4506-b073-017787e1c88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    # Set seeds for Python's random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set seed for TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "  \n",
    "\n",
    "# Define the LSTM model\n",
    "class LstmModel(keras.Model):\n",
    "    def __init__(self, input_shape, lstm_layers=3, no_units=3, output_units=1, dropout_rate=0.2, mean_response=0):\n",
    "        super(LstmModel, self).__init__()\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.no_units = no_units\n",
    "        self.output_units = output_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mean_response = mean_response\n",
    "\n",
    "        # Define LSTM and Dense layers\n",
    "        self.lstm_layers_list = [\n",
    "            LSTM(units=no_units, return_sequences=(i < lstm_layers - 1), \n",
    "                 kernel_initializer='zeros', recurrent_initializer='zeros', bias_initializer='zeros')\n",
    "            for i in range(lstm_layers)\n",
    "        ]\n",
    "        self.dense = Dense(units=output_units, kernel_initializer='zeros', \n",
    "                           bias_initializer=tf.keras.initializers.Constant(mean_response))\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        #LSTM layers\n",
    "        x = inputs\n",
    "        \n",
    "        for lstm_layer in self.lstm_layers_list:\n",
    "            x = lstm_layer(x)\n",
    "            \n",
    "        outputs = self.dense(x)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def summary(self):\n",
    "        super(LstmModel, self).summary()\n",
    "    \n",
    "    def compile(self, optimizer='adam', loss='mse', metrics=['mse'], learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Dictionary to map optimizer names to their classes\n",
    "        optimizers = {\n",
    "            'adam': Adam,\n",
    "            'nadam': Nadam,\n",
    "            'rms': RMSprop\n",
    "        }\n",
    "\n",
    "        # Get the optimizer class from the dictionary\n",
    "        optimizer_class = optimizers.get(optimizer.lower(), Adam)\n",
    "\n",
    "        # Instantiate the optimizer with the specified learning rate\n",
    "        optimizer_instance = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "        # Compile the model with the chosen optimizer, loss, and metrics\n",
    "        tf.config.run_functions_eagerly(True)\n",
    "        super(LstmModel, self).compile(optimizer=optimizer_instance, loss=loss, metrics=metrics)\n",
    "        \n",
    "    def fit(self, dataset, epochs=10, batch_size=32):\n",
    "        # Set up MLflow experiment\n",
    "        mlflow.set_experiment('LSTM_Experiment')\n",
    "        \n",
    "        # Shuffle and batch the dataset\n",
    "        dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "\n",
    "        # Split the dataset\n",
    "        val_size = 5\n",
    "        val_dataset = dataset.take(val_size)\n",
    "        train_dataset = dataset.skip(val_size)        \n",
    "\n",
    "        # Start MLflow run\n",
    "        with mlflow.start_run():\n",
    "            # Early stopping callback\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "            # Create a history callback\n",
    "            history_callback = tf.keras.callbacks.History()\n",
    "           \n",
    "            # Train the model\n",
    "            history = super(LstmModel, self).fit(train_dataset, epochs=epochs, batch_size=batch_size,\n",
    "                           validation_data=val_dataset, callbacks=[early_stopping, history_callback])\n",
    "\n",
    "            # Log model and parameters to MLflow\n",
    "            mlflow.keras.log_model(self, 'model')\n",
    "            mlflow.log_param('epochs', epochs)\n",
    "            mlflow.log_param('batch_size', batch_size)\n",
    "            mlflow.log_param('lstm_layers', self.lstm_layers)\n",
    "            mlflow.log_param('learning_rate', self.learning_rate)\n",
    "            \n",
    "            # Plot training progress\n",
    "            self.plot_training_progress(history)            \n",
    "\n",
    "            # Evaluate the model\n",
    "            loss = super(LstmModel, self).evaluate(val_dataset)\n",
    "            mlflow.log_metric('val_loss', loss)\n",
    "\n",
    "    def plot_training_progress(self, history):\n",
    "        # Plot training and validation loss\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a12278-010a-4b9f-881d-99afc573a007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.7283\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 00:39:05.173497: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-10-23 00:39:05.173790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LstmModel(input_shape\u001b[38;5;241m=\u001b[39minput_shape, lstm_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, no_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mean_response\u001b[38;5;241m=\u001b[39mmean_response)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 102\u001b[0m, in \u001b[0;36mLstmModel.fit\u001b[0;34m(self, dataset, epochs, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m history_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mHistory()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLstmModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m               \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Log model and parameters to MLflow\u001b[39;00m\n\u001b[1;32m    106\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlog_model(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1697\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1695\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1698\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1699\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1704\u001b[0m     )\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[1;32m   1706\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for _, response in dataset:\n",
    "    responses.append(response.numpy())\n",
    "mean_response = np.mean(responses)\n",
    "print(mean_response)\n",
    "\n",
    "model = LstmModel(input_shape=input_shape, lstm_layers=7, no_units=5, mean_response=mean_response)\n",
    "model.compile(optimizer='adam')\n",
    "model.fit(dataset, epochs=100, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f956d-ca74-46df-8375-ce6865465031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"orange\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd124-4055-422d-9134-cea12941d906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
