{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72aa9012-c901-475d-af55-d2d8279e3b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install --quiet --upgrade pip\n",
    "\n",
    "# Install the dependencies.\n",
    "!pip install --quiet -r gdal_req.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b51ceda-8485-4beb-83c5-1f92ee2a19f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the runtime by ending the process.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d93dbd-4fd4-4d22-a4fb-bf2715b1f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from serving.constants import  SCALE, BUCKET, IMG_SOURCE_PREFIX, HIST_DEST_PREFIX, HIST_BINS_LIST, SCALE, CROP, MONTHS, IMAGE_BATCH, hist_bins, SELECTED_BANDS, MAP_NAN, NORMALIZE, NUM_BINS, BANDS, get_bins_bands\n",
    "from serving.hist_data import recombine_image, write_histogram_to_gcs\n",
    "from serving.common import list_blobs_with_prefix\n",
    "from serving.data import get_varied_labels, get_labels\n",
    "import io\n",
    "import google.auth\n",
    "from rasterio.io import MemoryFile\n",
    "from osgeo import gdal\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2971217-5d41-48ec-af03-7b960f33e742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "bucket_name = BUCKET\n",
    "bins_list = HIST_BINS_LIST\n",
    "hist_buckets = NUM_BINS\n",
    "\n",
    "map_nan = MAP_NAN #Replace nan by 0 (False), mask (True)\n",
    "normalize = NORMALIZE #Divide by 10 000 reflectance scaling bringign values roughly to interval [0,1.6]\n",
    "\n",
    "directory_prefix = IMG_SOURCE_PREFIX\n",
    "output_prefix = HIST_DEST_PREFIX\n",
    "\n",
    "immgs_to_check =  {\"count_start\":0,\n",
    "                   \"no_records\":2000, # total num of possible choices ~1900 above 2000 means all images\n",
    "                   \"ascending\": False} \n",
    "months = MONTHS\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(filename=\"hist.log\",level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5124b61-1e2f-4ceb-93ed-a10f24ce1abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_name_composer(county, state_fips, year, month):\n",
    "    image_name = f\"{IMG_SOURCE_PREFIX}/{SCALE}/{county.capitalize()}_{state_fips}/{year}/{month}-{month+1}\"\n",
    "    return image_name\n",
    "\n",
    "def check_blob_prefix_exists(bucket_name, prefix):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    hist_blob = bucket.blob(prefix)\n",
    "    return hist_blob.exists()\n",
    "\n",
    "def batch_check_blobs(bucket_name, prefixes):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_prefix = {executor.submit(check_blob_prefix_exists, bucket_name, prefix): prefix for prefix in prefixes}\n",
    "        results = {}\n",
    "        for future in as_completed(future_to_prefix):\n",
    "            prefix = future_to_prefix[future]\n",
    "            results[prefix] = future.result()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cb7ed3-05b6-49d0-ba57-ecf007145ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_set = [\n",
    "            #[False, True, True, True, True, True, True, True, True, True, False, False, False],\n",
    "            [True, True, True, True, True, True, True, True, True, True, False, True, True]\n",
    "           ]\n",
    "bukcets_set = [60]\n",
    "map_nan = [True,]\n",
    "           #False]\n",
    "\n",
    "combinations = list(product(bukcets_set, band_set, map_nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f47f0-b4fe-4112-bd3c-45ac1a3fcad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images matching the name pattern: 20697\n",
      "Number of items to process: 18595\n",
      "\n",
      "    Total run time: 9.2 minutes\n",
      "    Check bucket: 5.5e+02 seconds\n",
      "    Generate list of missing histograms: 0.06 secods\n",
      "    \n",
      "18595\n",
      "images/60/Alpena_26/2016/5-6 -> histograms/nan_map_True/norm_True/60_buckets_12_bands/60/Alpena_26/2016/5-6\n",
      "Created 100 histograms already\n",
      "Created 200 histograms already\n",
      "Created 300 histograms already\n",
      "Created 400 histograms already\n",
      "Created 500 histograms already\n",
      "Created 600 histograms already\n",
      "Created 700 histograms already\n",
      "Created 800 histograms already\n",
      "Created 900 histograms already\n",
      "Created 1000 histograms already\n",
      "Created 1100 histograms already\n",
      "Created 1200 histograms already\n",
      "Created 1300 histograms already\n",
      "Created 1400 histograms already\n",
      "Created 1500 histograms already\n",
      "Created 1600 histograms already\n",
      "Created 1700 histograms already\n",
      "Created 1800 histograms already\n",
      "Created 1900 histograms already\n",
      "Created 2000 histograms already\n",
      "Created 2100 histograms already\n",
      "Created 2200 histograms already\n",
      "Created 2300 histograms already\n",
      "Created 2400 histograms already\n",
      "Created 2500 histograms already\n",
      "Created 2600 histograms already\n",
      "Created 2700 histograms already\n",
      "Created 2800 histograms already\n",
      "Created 2900 histograms already\n",
      "Created 3000 histograms already\n",
      "Created 3100 histograms already\n",
      "Created 3200 histograms already\n",
      "Created 3300 histograms already\n",
      "Created 3400 histograms already\n",
      "Created 3500 histograms already\n",
      "Created 3600 histograms already\n",
      "Created 3700 histograms already\n",
      "Created 3800 histograms already\n",
      "Created 3900 histograms already\n",
      "Created 4000 histograms already\n",
      "Created 4100 histograms already\n",
      "Created 4200 histograms already\n",
      "Created 4300 histograms already\n",
      "Created 4400 histograms already\n",
      "Created 4500 histograms already\n",
      "Created 4600 histograms already\n",
      "Created 4700 histograms already\n",
      "Created 4800 histograms already\n",
      "Created 4900 histograms already\n",
      "Created 5000 histograms already\n",
      "Created 5100 histograms already\n",
      "Created 5200 histograms already\n",
      "Created 5300 histograms already\n",
      "Created 5400 histograms already\n",
      "Created 5500 histograms already\n",
      "Created 5600 histograms already\n",
      "Created 5700 histograms already\n",
      "Created 5800 histograms already\n",
      "Created 5900 histograms already\n",
      "Created 6000 histograms already\n",
      "Created 6100 histograms already\n",
      "Created 6200 histograms already\n",
      "Created 6300 histograms already\n",
      "Created 6400 histograms already\n",
      "Created 6500 histograms already\n",
      "Created 6600 histograms already\n",
      "Created 6700 histograms already\n",
      "Created 6800 histograms already\n",
      "Created 6900 histograms already\n",
      "Created 7000 histograms already\n"
     ]
    }
   ],
   "source": [
    "# # Create a few sets of histograms\n",
    "#     - vary sample size\n",
    "#     - number of buckets\n",
    "#     - number of bands \n",
    "\n",
    "for bin_band_combo_map_nan in combinations:\n",
    "    n_bins, band_selector, map_nan = bin_band_combo_map_nan\n",
    "    hist_buckets, sel_bands = get_bins_bands(n_bins,band_selector).values()\n",
    "    \n",
    "    def blob_name_composer(county, state_fips, year, month, map_nan, normalize):\n",
    "        blob_name = f\"{HIST_DEST_PREFIX}/nan_map_{map_nan}/norm_{normalize}/{n_bins}_buckets_{len(sel_bands)}_bands/{SCALE}/{county.capitalize()}_{state_fips}/{year}/{month}-{month+1}\"\n",
    "        return blob_name    \n",
    "    \n",
    "    # Generate all prefixes\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    labels_df = get_varied_labels(**immgs_to_check)\n",
    "    labels = list(zip(labels_df[\"county_name\"],\n",
    "                      labels_df[\"county_ansi\"],\n",
    "                 labels_df[\"state_ansi\"],\n",
    "                 labels_df[\"year\"]))\n",
    "\n",
    "    prefixes = [f'images/{SCALE}/{county.capitalize()}_{fips}/{year}/{month}-{month+1}.tif' \n",
    "                for county,_, fips, year in labels\n",
    "                for month in months]\n",
    "\n",
    "    prefixes_hist = [f'histograms/nan_map_{map_nan}/norm_{normalize}/{n_bins}_buckets_{len(sel_bands)}_bands/{SCALE}/{county.capitalize()}_{fips}/{year}/{month}-{month+1}.npy' \n",
    "                for county,_, fips, year in labels\n",
    "                for month in months]\n",
    "\n",
    "    generate_prefixes = time.perf_counter()\n",
    "\n",
    "    # Batch check all prefixes\n",
    "    results_img = batch_check_blobs(bucket_name, prefixes)\n",
    "    results_hist = batch_check_blobs(bucket_name, prefixes_hist)\n",
    "\n",
    "    check_bucket = time.perf_counter()\n",
    "\n",
    "    # Generate get_input_img_params based on results\n",
    "    get_input_img_params = [\n",
    "        {\"county\": county.capitalize(), \"state_fips\": fips, \"year\": year, \"month\": month}\n",
    "                for county, county_fips, fips, year in labels\n",
    "                for month in months\n",
    "                if (results_img[f'images/{SCALE}/{county.capitalize()}_{fips}/{year}/{month}-{month+1}.tif'] and\n",
    "                    not results_hist[f'histograms/nan_map_{map_nan}/norm_{normalize}/{n_bins}_buckets_{len(sel_bands)}_bands/{SCALE}/{county.capitalize()}_{fips}/{year}/{month}-{month+1}.npy'])\n",
    "    ]\n",
    "\n",
    "    generate_valid_list = time.perf_counter()\n",
    "    print(f\"Number of images matching the name pattern: {sum(results_img.values())}\")\n",
    "    print(f\"Number of items to process: {len(get_input_img_params)}\")\n",
    "    print(f\"\"\"\n",
    "    Total run time: {(generate_valid_list - start_time)/60:.02} minutes\n",
    "    Check bucket: {check_bucket - generate_prefixes:.02} seconds\n",
    "    Generate list of missing histograms: {generate_valid_list - check_bucket:.02} secods\n",
    "    \"\"\")\n",
    "\n",
    "    images_to_process = [img_name_composer(**params) for params in get_input_img_params]\n",
    "    blob_names = [blob_name_composer(**params, map_nan = map_nan, normalize = normalize) for params in get_input_img_params]\n",
    "    print(len(blob_names))\n",
    "    print(images_to_process[2],blob_names[2], sep=\" -> \")\n",
    "\n",
    "    # Usage\n",
    "    start_time = time.perf_counter()\n",
    "    count = 0\n",
    "    for image_name, blob_name in zip(images_to_process, blob_names):\n",
    "        recombine_image_hist = recombine_image(BUCKET, image_name, hist_buckets, sel_bands, MAP_NAN, NORMALIZE)\n",
    "        write_histogram_to_gcs(recombine_image_hist, BUCKET, blob_name)\n",
    "        count += 1 \n",
    "        if count % 100 == 0:\n",
    "            print(f\"Created {count} histograms already\")\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    print(f\"Finished, created {count} histograms\")\n",
    "    print(f\"Elapsed time {(end_time - start_time)/60:.02} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f602721-7b39-48a9-9bc0-f0eaea681794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-gdal_conda-gdal_conda",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "GDAL_kernel (Local)",
   "language": "python",
   "name": "conda-env-gdal_conda-gdal_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
